{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Lab6_Mapo.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"da26c8c554b246508a59af527f1c22b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4ab58fa0566643e586a6153c1f608b81","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c3a8c24d9a8c488b914f0b1cbe3d4c12","IPY_MODEL_495d8472cbcd49edbc71abbc507c900a"]}},"4ab58fa0566643e586a6153c1f608b81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c3a8c24d9a8c488b914f0b1cbe3d4c12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4d2286f59d66406d937dabdfb7491252","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":52147035,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":52147035,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_790bff0575e34d7f9452622d4f4bb848"}},"495d8472cbcd49edbc71abbc507c900a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f400d98ccef4532b7077ace8f51b324","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 49.7M/49.7M [00:02&lt;00:00, 23.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8a8824cf75e345398b648cf3f50b2431"}},"4d2286f59d66406d937dabdfb7491252":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"790bff0575e34d7f9452622d4f4bb848":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f400d98ccef4532b7077ace8f51b324":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8a8824cf75e345398b648cf3f50b2431":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TfOmH70u5Ayv"},"source":["# AUTOMATIC DIAGNOSTIC SYSTEM OF SKIN LESSIONS FROM DERMOSCOPIC IMAGES\n","\n","\n","In this practice we are going to build a skin lesion diagnosis system based on dermoscopic image analysis.\n","\n","## Part 0: The problem\n","\n","Before starting the practice, we will briefly describe the database that we will use and the problem we aim to address:\n","\n","Our goal is to develop a CNN providing an automatic diagnosis of cutaneous diseases from dermoscopic images. Dermoscopy is a non-invasive technique that allows the evaluation of the colors and microstructures of the epidermis, the dermoepidermal joint and the papillary dermis that are not visible to the naked eye. These structures are specifically correlated with histological properties of the lesions. Identifying specific visual patterns related to color distribution or dermoscopic structures can help dermatologists decide the malignancy of a pigmented lesion. The use of this technique provides a great help to the experts to support their diagnosis. However, the complexity of its analysis limits its application to experienced clinicians or dermatologists.\n","\n","In our scenario, we will consider 3 classes of skin lesions:\n","\n","- Malignant melanoma: Melanoma, also known as malignant melanoma, is the most common type of cancer, and arises from pigmented cells known as melanocytes. Melanomas typically occur on the skin and rarely elsewhere such as the mouth, intestines, or eye.\n","\n","- Seborrheic keratosis: it is a noncancerous (benign) tumor of the skin that originates from the cells of the outer layer of the skin (keranocytes), so it is a non-melanocytic lesion.\n","\n","- Benign nevus: a benign skin tumor caused by melanocytes (it is melanocytic)\n","\n","Figure 1 shows a visual example of the 3 considered lesions:\n","\n","![Image of ISIC](http://www.tsc.uc3m.es/~igonzalez/images/ISIC.jpg)\n","\n","The dataset has been obtained from the 'Internatial Skin Imaging Collaboration' (ISIC) file. It contains 2750 images divided into 3 sets:\n","- Training set: 2000 images\n","- Validation set: 150 images\n","- Test set: 600 images\n","\n","For each clinical case, two images are available:\n","- The dermoscopic image of the lesion (in the ‘images’ folder).\n","- A binary mask with the segmentation between injury (mole) and skin (in the 'masks' folder)\n","\n","Additionally, there is a csv file for each dataset (training, validation and test) in which each lines corresponds with a clinical case, defined with two fields separated by commas:\n","- the numerical id of the lesion: that allows to build the paths to the image and mask.\n","- the lesion label: available only for training and validation, being an integer between 0 and 2: 0: benign nevus, 1: malignant melanoma, 2: seborrheic keratosis. In the case of the test set, labels are not available (their value is -1).\n","\n","Students will be able to use the training and validation sets to build their solutions and finally provide the scores associated with the test set. This practice provides a guideliness to build a baseline reference system. To do so, we will learn two fundamental procedures:\n","\n","- 1) Process your own database with pytorch\n","- 2) Fine-tuning a regular network for our diagnostic problem\n","\n","## Part 1: Handling our custom dataset with pytorch\n","Now we are going to study how we can load and process our custom dataset in pytorch. For that end, we are going to use the package ``scikit-image`` for reading images, and the package ``panda`` for reading csv files.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1589194080161,"user_tz":-120,"elapsed":5005,"user":{"displayName":"Ignacio Serrano LLabrés","photoUrl":"","userId":"08869020757884954320"}},"id":"dE-enWtK5Ayw","outputId":"d94174ab-3276-4814-869d-d5cba48a64a4","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from __future__ import print_function, division\n","import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","from sklearn import metrics\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils, models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import time\n","import copy\n","import pdb\n","from google.colab import files\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","from torch.utils.data import WeightedRandomSampler\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.ion()   # interactive mode\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device) # not optimal cpu must be chosen GPU"],"execution_count":1,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S-tDZznG5Ayy"},"source":["Lo primero que vamos a hacer es descargar y descomprimir la base de datos a un directorio local de trabajo:\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1589194126751,"user_tz":-120,"elapsed":51577,"user":{"displayName":"Ignacio Serrano LLabrés","photoUrl":"","userId":"08869020757884954320"}},"id":"BUfCOAQz5Ayz","outputId":"2f25dc9b-d988-4ab0-d98d-2a979add8170","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["#ONLY TO USE GOOGLE COLAB. Run this code only the first time you run this notebook and then comment these lines\n","from shutil import copyfile\n","from google.colab import drive\n","import os, sys\n","\n","drive.mount('/content/drive')\n","copyfile('/content/drive/My Drive/University/UC3M_Master/2nd_Semester/Computer Vision/Labs/Lab6/db1.zip', './db1.zip') #Copy db files to our working folder\n","copyfile('/content/drive/My Drive/University/UC3M_Master/2nd_Semester/Computer Vision/Labs/Lab6/db2.zip', './db2.zip')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'./db2.zip'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JwHQ2Rfv5Ay2","colab":{}},"source":["#NOTE: Run this only once, in the machine where you want to run your code, then comment these lines\n","import zipfile\n","zipPath='./db1.zip' #path of the 1st zip file\n","dataFolder='./data' #We extract files to the current folder\n","with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n","    zip_ref.extractall(dataFolder)\n","    \n","zipPath='./db2.zip' #path of the 2nd zip file\n","dataFolder='./data' # We extract files to the current folder\n","with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n","    zip_ref.extractall(dataFolder)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_185EoW85Ay5"},"source":["Now let's read the indexed file and display data from image 65. The file structure is one row per image of the database, and two fields:\n","- Image ID (a 4-digit sequence, adding 0 to the left side if required)\n","- Label: 0 benign nevus, 1 melanoma, 2 seborrheic keratosis\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S2SAspn15Ay6","colab":{}},"source":["#db = pd.read_csv('data/dermoscopyDBtrain.csv',header=0,dtype={'id': str, 'label': int})\n","\n","#We show inform\n","#n = 65\n","#img_id = db.id[65] \n","#label = db.label[65]\n","\n","\n","#print('Image ID: {}'.format(img_id))\n","#print('Label: {}'.format(label))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Qc86aHK5AzA"},"source":["Now, let's create a simple function to show an image.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PLfdDx_C5AzA","colab":{}},"source":["def imshow(image, title_str):\n","    if len(image.shape)>2:\n","        plt.imshow(image)\n","    else:\n","        plt.imshow(image,cmap=plt.cm.gray)\n","    plt.title(title_str)        \n","\n","#plt.figure()\n","#imshow(io.imread(os.path.join('data/images/', img_id + '.jpg' )),'Image %d'%n)\n","#plt.figure()\n","#imshow(io.imread(os.path.join('data/masks/', img_id + '.png')),'Mask %d'%n)\n","\n","#plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HRS7nEp35AzH"},"source":["### Class Dataset\n","\n","The class `` torch.utils.data.Dataset`` is an abstract class that represents a dataset.\n","\n","To create our custom dataset in pytorch we must inherit from this class and overwrite the following methods:\n","\n","- `` __len__`` so that `` len (dataset) `` returns the size of the dataset.\n","- `` __getitem__`` to support indexing `` dataset [i] `` when referring to sample $i$\n","\n","We are going to create the train and test datasets of our diagnostic problem. We will read the csv in the initialization method `` __init__`` but we will leave the explicit reading of the images for the method\n","`` __getitem__``. This approach is more efficient in memory because all the images are not loaded in memory at first, but are read individually when necessary.\n","\n","Our dataset is going to be a dictionary `` {'image': image, 'mask': mask, 'label': label} ``. You can also take an optional `` transform '' argument so that we can add pre-processing and data augmentation techniques.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Tr8dfyq2XQLD","colab":{}},"source":["def load_data (data_dir, dataset): \n","    \n","#   dataset = pd.read_csv(csv_file,header=0, dtype={'id': str, 'label': int})\n","    images = []\n","    masks = []\n","    labels = []\n","    for i,idx in enumerate(dataset.id):\n","        #Leemos la imagen\n","        img_name = os.path.join(data_dir+'/images', idx + '.jpg')\n","        image = io.imread(img_name)\n","        #Leemos la máscara\n","        mask_name = os.path.join(data_dir+'/masks', idx + '.png')\n","        mask = io.imread(mask_name)\n","        label = dataset.label[i]\n","        \n","        images.append(image)\n","        masks.append(mask)\n","        labels.append(label)\n","        \n","    \n","    \n","    return images, masks, labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r5T8RIhm5AzJ","colab":{}},"source":["class DermoscopyDataset(Dataset):\n","    \"\"\"Dermoscopy dataset.\"\"\"\n","\n","    def __init__(self, csv_file, root_dir, transform=None, reducedSize=False):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path al fichero csv con las anotaciones.\n","            root_dir (string): Directorio raíz donde encontraremos las carpetas 'images' y 'masks' .\n","            transform (callable, optional): Transformaciones opcionales a realizar sobre las imágenes.\n","        \"\"\"\n","        dataset = pd.read_csv(csv_file,header=0,dtype={'id': str, 'label': int})\n","        \n","        \n","\n","        if reducedSize:\n","          idx=np.random.permutation(range(len(dataset)))\n","          reduced_dataset=dataset.iloc[idx[0:reducedSize]]\n","          dataset=reduced_dataset.reset_index(drop=True)\n","#          \n","        self.imgs, self.masks, self.labels = load_data (root_dir, dataset)\n","#        self.root_dir = root_dir\n","#        self.img_dir = os.path.join(root_dir,'images') \n","#        self.mask_dir = os.path.join(root_dir,'masks')\n","        self.transform = transform\n","        self.classes = ['nevus', 'melanoma', 'keratosis']\n","        \n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","#        #Leemos la imagen\n","#        img_name = os.path.join(self.img_dir,self.dataset.id[idx] + '.jpg')\n","#        image = io.imread(img_name)\n","#        #Leemos la máscara\n","#        mask_name = os.path.join(self.mask_dir,self.dataset.id[idx] + '.png')\n","#        mask = io.imread(mask_name)\n","        image = self.imgs[idx]\n","        label = self.labels[idx]\n","        mask = self.masks[idx]\n","        \n","        sample = {'image': image, 'mask': mask, 'label':  label}\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3dvEWxd95AzL"},"source":["We now instantiate the class to iterate over some samples to see what we generate.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P4Xm3ZHW5AzM","scrolled":true,"colab":{}},"source":["#train_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtrain.csv', root_dir='data')\n","\n","#fig = plt.figure()\n","\n","#for i in range(len(train_dataset)):\n","#    sample = train_dataset[i]\n","#    print(i, sample['image'].shape, sample['label'])\n","\n","    #ax = plt.subplot(1, 4, i + 1)\n","    #plt.tight_layout()\n","    #ax.set_title('Sample #{}'.format(i))\n","    #ax.axis('off')\n","    #plt.imshow(sample['image'])\n","\n","    #if i == 3:\n","    #    plt.show()\n","    #    break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"amIkO4cC8lQh","colab_type":"code","colab":{}},"source":["import random\n","import pandas as pd\n","import numpy as np\n","\n","# Here we subdivide the train and test set from the main dataset\n","# So we can validate better the parameters\n","# Finally we will train completelly with the whole dataset\n","\n","def getTestSet(tr_set, percent):\n","\n","  rows_tst = int(tr_set.shape[0]*percent)\n","  \n","  tst_set = tr_set.sample(n = rows_tst) \n","  tr_set = tr_set[~tr_set['id'].isin(tst_set['id'])]\n","\n","  tst_set['id'] = pd.Series([str(val).zfill(4) for val in tst_set['id']], index = tst_set.index)\n","  tr_set['id'] = pd.Series([str(val).zfill(4) for val in tr_set['id']], index = tr_set.index)\n","\n","  print(np.intersect1d(tst_set['id'], tr_set['id']))\n","\n","  return tr_set, tst_set"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_KE_2Lvw7JT-","colab_type":"code","colab":{}},"source":["dataset = pd.read_csv('data/dermoscopyDBtrain.csv')\n","val_dataset = pd.read_csv('data/dermoscopyDBval.csv')\n","test_dataset = pd.read_csv('data/dermoscopyDBtest.csv')\n","\n","#train ,test = getTestSet(dataset, 0.1)\n","\n","#train.to_csv( '/content/drive/My Drive/University/UC3M_Master/2nd_Semester/Computer Vision/Labs/Lab6/DBtrain_1.csv', index=False)\n","#test.to_csv('/content/drive/My Drive/University/UC3M_Master/2nd_Semester/Computer Vision/Labs/Lab6/DBtest_1.csv', index=False)\n","\n","#tr_path = '/content/drive/My Drive/University/UC3M_Master/2nd_Semester/Computer Vision/Labs/Lab6/DBtrain_1.csv'\n","#tst_path = '/content/drive/My Drive/University/UC3M_Master/2nd_Semester/Computer Vision/Labs/Lab6/DBtest_1.csv'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8ZqZyyPh5AzO"},"source":["### Transforms\n","----------\n","\n","In the previously shown examples we can see that the size of the images is not the same. This would prevent to train a red convolutional neuronal, as the vast majority require fixed-size inputs. Furthermore, the image is not always adjusted to the lesion, and indeed, in some examples lesions are very small compared to the size of the image. It would then be desirable to adjust the input images so that the lesion covers almost the entire image.\n","\n","To do this, we are going to create some preprocessing code, focusing on 4 transformations:\n","\n","- `` CropByMask``: to crop the image using the lesion mask\n","- `` Rescale``: to scale the image\n","- `` RandomCrop``: to crop the image randomly, it allows us to augment the data samples with random crops\n","- `` ToTensor``: to convert numpy matrices into torch tensors (rearranging the axes).\n","\n","We will define them as callable classes instead of simple functions, as we will not need to pass the transform  parameters every time we call a method. To do this, we only have to implement the `` __call__`` method and, if necessary, the `` __init__`` method.\n","Then we can use a transformation with the following code:\n","\n","::\n","\n","    tsfm = Transform(params)\n","    transformed_sample = tsfm(sample)\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nZjKbEw75AzP","colab":{}},"source":["class CropByMask(object):\n","    \"\"\"Crop the image using the lesion mask.\n","\n","    Args:\n","        border (tuple or int): Border surrounding the mask. We dilate the mask as the skin surrounding \n","        the lesion is important for dermatologists.\n","        If it is a tuple, then it is (bordery,borderx)\n","    \"\"\"\n","\n","    def __init__(self, border):\n","        assert isinstance(border, (int, tuple))\n","        if isinstance(border, int):\n","            self.border = (border,border)\n","        else:\n","            self.border = border\n","            \n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        h, w = image.shape[:2]\n","        #Calculamos los índices del bounding box para hacer el cropping\n","        sidx=np.nonzero(mask)\n","        minx=np.maximum(sidx[1].min()-self.border[1],0)\n","        maxx=np.minimum(sidx[1].max()+1+self.border[1],w)\n","        miny=np.maximum(sidx[0].min()-self.border[0],0)\n","        maxy=np.minimum(sidx[0].max()+1+self.border[1],h)\n","        #Recortamos la imagen\n","        image=image[miny:maxy,minx:maxx,...]\n","        mask=mask[miny:maxy,minx:maxx]\n","\n","        return {'image': image, 'mask': mask, 'label' : label}\n","    \n","class Rescale(object):\n","    \"\"\"Re-scale image to a predefined size.\n","\n","    Args:\n","        output_size (tuple or int): The desired size. If it is a tuple, output is the output_size. \n","        If it is an int, the smallest dimension will be the output_size\n","            a we will keep fixed the original aspect ratio.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","\n","        h, w = image.shape[:2]\n","        if isinstance(self.output_size, int):\n","            if h > w:\n","                new_h, new_w = self.output_size * h / w, self.output_size\n","            else:\n","                new_h, new_w = self.output_size, self.output_size * w / h\n","        else:\n","            new_h, new_w = self.output_size\n","\n","        new_h, new_w = int(new_h), int(new_w)\n","\n","        img = transform.resize(image, (new_h, new_w))\n","        msk = transform.resize(mask, (new_h, new_w))\n","\n","        return {'image': img, 'mask': msk, 'label' : label}\n","\n","\n","class RandomCrop(object):\n","    \"\"\"Randomly crop the image.\n","\n","    Args:\n","        output_size (tuple or int): Crop size. If  int, square crop\n","\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","\n","        if h>new_h:\n","            top = np.random.randint(0, h - new_h)\n","        else:\n","            top=0\n","            \n","        if w>new_w: \n","            left = np.random.randint(0, w - new_w)\n","        else:\n","            left = 0\n","            \n","        image = image[top: top + new_h,\n","                     left: left + new_w]\n","\n","        mask = mask[top: top + new_h,\n","                      left: left + new_w]\n","\n","\n","        return {'image': image, 'mask': mask, 'label': label}\n","\n","\n","class ToTensor(object):\n","    \"\"\"Convert ndarrays into pytorch tensors.\"\"\"\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","\n","        # Cambiamos los ejes\n","        # numpy image: H x W x C\n","        # torch image: C X H X W\n","        image = image.transpose((2, 0, 1))\n","        image = torch.from_numpy(image)\n","        # A la máscara le añadimos una dim fake al principio\n","        mask = torch.from_numpy(mask)\n","        mask = mask.unsqueeze(0)\n","        label=torch.tensor(label,dtype=torch.long)\n","        \n","        return {'image':image,\n","                'mask':mask,\n","                'label':label}\n","    \n","class Normalize(object):\n","    \"\"\"Normalize data by subtracting means and dividing by standard deviations.\n","\n","    Args:\n","        mean_vec: Vector with means. \n","        std_vec: Vector with standard deviations.\n","    \"\"\"\n","\n","    def __init__(self, mean,std):\n","      \n","        assert len(mean)==len(std),'Length of mean and std vectors is not the same'\n","        self.mean = np.array(mean)\n","        self.std = np.array(std)\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        c, h, w = image.shape\n","        assert c==len(self.mean), 'Length of mean and image is not the same' \n","        dtype = image.dtype\n","        mean = torch.as_tensor(self.mean, dtype=dtype, device=image.device)\n","        std = torch.as_tensor(self.std, dtype=dtype, device=image.device)\n","        image.sub_(mean[:, None, None]).div_(std[:, None, None])\n","    \n","        \n","        return {'image': image, 'mask': mask, 'label' : label}\n","\n","class RandomHorizontalFlip(object):\n","    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n","\n","    Args:\n","        p (float): probability of the image being flipped. Default value is 0.5\n","    \"\"\"\n","\n","    def __init__(self, p=0.5):\n","        self.p = p\n","\n","    def __call__(self, sample):\n","        \n","        img = sample['image']\n","        mask = sample['mask']\n","        label = sample['label']\n","        \n","        if random.random() < self.p:\n","            return {'image': np.fliplr(img), 'mask': np.fliplr(mask), 'label': label}\n","        return sample\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(p={})'.format(self.p)\n","    \n","class RandomVerticalFlip(object):\n","    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n","\n","    Args:\n","        p (float): probability of the image being flipped. Default value is 0.5\n","    \"\"\"\n","\n","    def __init__(self, p=0.5):\n","        self.p = p\n","\n","    def __call__(self, sample):\n","        \n","        img = sample['image']\n","        mask = sample['mask']\n","        label = sample['label'] \n","        \n","        if random.random() < self.p:\n","            return {'image':np.flipud(img), 'mask': np.flipud(mask), 'label': label}\n","        return sample\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(p={})'.format(self.p)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cDsuosPS5AzR"},"source":["### Composed Transforms\n","\n","Now let's apply the different transformations to our images. \n","\n","We will rescale the images so that their smallest dimension is 256 and then make random crops of size 224. To compose the transformations ``Rescale`` and ``RandomCrop`` we can use ``torchvision.transforms.Compose``, which is a simple callable class.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IwZV43mN5AzS","colab":{}},"source":["#cmask = CropByMask(15)\n","#scale = Rescale(256)\n","#crop = RandomCrop(224)\n","#composed = transforms.Compose([CropByMask(15), Rescale(256),RandomCrop(224)])\n","\n","# Apply each of the above transforms on sample.\n","#fig = plt.figure()\n","#sample = train_dataset[65]\n","#ax = plt.subplot(2,3, 1)\n","#plt.tight_layout()\n","#ax.set_title('original')\n","#plt.imshow(sample['image'])\n","    \n","#for i, tsfrm in enumerate([cmask, scale, crop, composed]):\n","    #transformed_sample = tsfrm(sample)\n","\n","    #ax = plt.subplot(2, 3, i + 2)\n","    #plt.tight_layout()\n","    #ax.set_title(type(tsfrm).__name__)\n","    #plt.imshow(transformed_sample['image'])\n","\n","#plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4OCMRDsm5AzX"},"source":["Iterating the dataset\n","-----------------------------\n","\n","We can now put everything together to create the train and test datasets with the corresponding transformations.\n","In summary, every time we sample an image from the dataset (during training):\n","- We will read the image and the mask\n","- We will apply the transformations and we will crop the image using a bounding box computed from the mask\n","- As the final cropping operation is random, we perform data augmentation during sampling\n","\n","We can easily iterate over the dataset with a ``for i in range`` loop.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7B_iTK9C5AzY","colab":{}},"source":["def getDermoData(train_p, test_p):\n","\n","  my_transforms = transforms.Compose([CropByMask(15),\n","                                    RandomVerticalFlip(),\n","                                    RandomHorizontalFlip(),\n","                                    Rescale(224),\n","                                    RandomCrop(224),\n","                                    ToTensor(),\n","                                    Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])\n","                                    ])\n","  #Train Dataset\n","  train_dataset = DermoscopyDataset(csv_file=train_p,\n","                                      root_dir='data',\n","                                      transform=my_transforms)\n","\n","  #Test dataset\n","  test_dataset = DermoscopyDataset(csv_file=test_p,\n","                                      root_dir='data',\n","                                      transform=transforms.Compose([\n","                                      CropByMask(15),\n","                                      Rescale((224,224)),\n","                                      ToTensor(),\n","                                      Normalize(mean=[0.485, 0.456, 0.406],\n","                                      std=[0.229, 0.224, 0.225])\n","                                      ]))\n","\n","  return train_dataset, test_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kJqvwAe1Xgsb","colab":{}},"source":["#Train Dataset\n","train_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtrain.csv',\n","                                    root_dir='data',\n","                                    transform=transforms.Compose([\n","                                    CropByMask(15),\n","                                    Rescale(224),\n","                                    RandomCrop(224),\n","                                    ToTensor(),\n","                                    Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])\n","                                    ]))\n","#Val dataset\n","val_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBval.csv',\n","                                    root_dir='data',\n","                                    transform=transforms.Compose([\n","                                    CropByMask(15),\n","                                    Rescale(224),\n","                                    RandomCrop(224),\n","                                    ToTensor(),\n","                                    Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])\n","                                    ]))\n","\n","#Test dataset\n","test_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtest.csv',\n","                                    root_dir='data',\n","                                    transform=transforms.Compose([\n","                                    CropByMask(15),\n","                                    Rescale(224),\n","                                    RandomCrop(224),\n","                                    ToTensor(),\n","                                    Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])\n","                                    ]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ap55335G5Azb"},"source":["Finally, we have to create a dataloader allowing to:\n","\n","- Sample batches of samples to feed the network during training\n","- Shuffle data\n","- Load the data in parallel using multiple cores.\n","\n","``torch.utils.data.DataLoader`` is an iterator that provides all these features. An important parameter of the iterator is ``collate_fn``. We can specify how samples are organized in batches by choosing the most appropriate function. In any case, the default option should work fine in most cases.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OaOClDTT5Azb","scrolled":false,"colab":{}},"source":["def loadData(train_p, train_dataset, val_dataset, test_dataset, train_batch, testval_batch):\n","  #WEIGHTED SAMPLER\n","\n","  db = pd.read_csv(train_p,header=0,dtype={'id': str, 'label': int})\n","  target = db.label.to_numpy()\n","\n","  cls_weights = torch.from_numpy(compute_class_weight('balanced', np.unique(target), target))\n","  weights = cls_weights[target]\n","  sampler = WeightedRandomSampler(weights, len(target), replacement=True)\n","\n","  ##Specify training dataset, with a batch size of 8, shuffle the samples, and parallelize with 4 workers\n","  train_dataloader = DataLoader(train_dataset, batch_size=train_batch, sampler=sampler, drop_last=True)\n","\n","  #Validation dataset => No shuffle\n","  val_dataloader = DataLoader(val_dataset, batch_size=testval_batch, shuffle=False)\n","\n","  #Test Dataset => => No shuffle\n","  test_dataloader = DataLoader(test_dataset, batch_size=testval_batch, shuffle=False)\n","\n","  return train_dataloader, val_dataloader, test_dataloader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zIDXNa2U5Azd"},"source":["## Part 2: Fine-tuning a pre-trained model\n","\n","In the second part of the practice we will build an automatic skin lesion diagnosis system. Instead of training a CNN designed by us from the beginning, we will fine-tune a network that has previously been trained for another task. As seen in the lectures, this usually becomes a good alternative when we do not have many data in the training dataset (in relation to the parameters to be learned).\n","\n","In particular, we will use the resnet-18 CNN, included in the ``torchvision`` package.\n","\n","### Performance Metric for evaluation\n","We will start by defining the metric we will use to evaluate our network. In particular, and following the instructions of the organizers of the original ISIC challenge, we will use the area under the ROC or AUC, but we will calculate 3 different AUCs:\n","- 1) AUC of binary problem melanoma vs all\n","- 2) AUC of the binary problem seborrheic keratosis vs all\n","- 3) AUC average of the previous two\n","\n","The following function computes AUCs from the complete database outputs:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4syL0Zh35Aze","colab":{}},"source":["#Function that computes 2 AUCs: melanoma vs all and keratosis vs all\n","# scores is nx3: n is the number of samples in the dataset \n","# labels is nx1\n","# Function resturns an array with two elements: the auc values\n","def computeAUCs(scores,labels):\n","    #0: benign nevus, 1: malignant melanoma, 2: seborrheic keratosis.            \n","    aucs = np.zeros((3,))\n","    #Calculamos el AUC benign vs all\n","    scores_b = scores[:,0]\n","    labels_b = (labels == 0).astype(np.int) \n","    aucs[0]=metrics.roc_auc_score(labels_b, scores_b)\n","    \n","    #Calculamos el AUC melanoma vs all\n","    scores_mel = scores[:,1]\n","    labels_mel = (labels == 1).astype(np.int) \n","    aucs[1]=metrics.roc_auc_score(labels_mel, scores_mel)\n","\n","    #Calculamos el AUC queratosis vs all\n","    scores_sk = scores[:,2]\n","    labels_sk = (labels == 2).astype(np.int) \n","    aucs[2]=metrics.roc_auc_score(labels_sk, scores_sk)\n","    \n","    return aucs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lgvWRbVl5Azg"},"source":["### Training function\n","\n","We continue defining the function to train our classifier:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9VVqtkkl5Azg","colab":{}},"source":["def train_model(model, criterion, optimizer, scheduler, image_datasets, dataset_sizes,\n","                dataloaders, device, num_epochs=25):\n","    since = time.time()\n","    \n","    numClasses = len(image_datasets['train'].classes)\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_aucs = np.zeros((2,)) #AUCs melanoma vs all, and keratosis\n","    best_auc = 0\n","    #Loop of epochs (each iteration involves train and val datasets)\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        \n","        \n","        # Cada época tiene entrenamiento y validación\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set the model in training mode\n","            else:\n","                model.eval()   # Set the model in val mode (no grads)\n","\n","            \n","            #Dataset size\n","            numSamples = dataset_sizes[phase]\n","            \n","            # Create variables to store outputs and labels\n","            outputs_m=np.zeros((numSamples,numClasses),dtype=np.float)\n","            labels_m=np.zeros((numSamples,),dtype=np.int)\n","            running_loss = 0.0\n","            \n","            contSamples=0\n","            \n","            # Iterate (loop of batches)\n","            for sample in dataloaders[phase]:\n","                inputs = sample['image'].to(device).float()\n","                labels = sample['label'].to(device)\n","#                print('labels in a batch:',labels)\n","                \n","                \n","                #Batch Size\n","                batchSize = labels.shape[0]\n","                \n","                # Set grads to zero\n","                optimizer.zero_grad()\n","\n","                # Forward\n","                # Register ops only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward & parameters update only in train\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","                \n","                # Accumulate the running loss\n","                running_loss += loss.item() * inputs.size(0)\n","                \n","                #Apply a softmax to the output\n","                outputs=F.softmax(outputs.data,dim=1)\n","                # Store outputs and labels \n","                outputs_m [contSamples:contSamples+batchSize,...]=outputs.cpu().numpy()\n","                labels_m [contSamples:contSamples+batchSize]=labels.cpu().numpy()\n","                contSamples+=batchSize\n","                \n","            #At the end of an epoch, update the lr scheduler    \n","            if phase == 'train':\n","                scheduler.step()\n","            \n","            #Accumulated loss by epoch\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            \n","            #Compute the AUCs at the end of the epoch\n","            aucs=computeAUCs(outputs_m,labels_m)\n","            \n","            #And the Average AUC\n","            epoch_auc = aucs[1:].mean()\n","                         \n","            print('{} Loss: {:.4f} AUC mel: {:.4f} sk: {:.4f} avg: {:.4f}'.format(\n","                phase, epoch_loss, aucs[1], aucs[2], epoch_auc))\n","\n","            # Deep copy of the best model\n","            if phase == 'val' and epoch_auc > best_auc:\n","                best_auc = epoch_auc\n","                best_aucs = aucs.copy()        \n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val AUCs: mel {:4f} sk {:4f} avg {:4f}'.format(best_aucs[1],best_aucs[2],best_auc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Hup6tIU55Azi"},"source":["Include an auxiliary function to show some predictions:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sFeVzJAs5Azj","colab":{}},"source":["def visualize_model(model, num_images=6):\n","    was_training = model.training\n","    model.eval()\n","    images_so_far = 0\n","    fig = plt.figure()\n","\n","    with torch.no_grad():\n","        for i, sample in enumerate(dataloaders['val']):\n","            inputs = sample['image'].to(device).float()\n","            labels = sample['label'].to(device)\n","            \n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            for j in range(inputs.size()[0]):\n","                images_so_far += 1\n","                ax = plt.subplot(num_images//2, 2, images_so_far)\n","                ax.axis('off')\n","                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n","                imshow(inputs.cpu().data[j])\n","\n","                if images_so_far == num_images:\n","                    model.train(mode=was_training)\n","                    return\n","        model.train(mode=was_training)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XyG510Rowb1Y","colab":{}},"source":["def get_results(model, batchSize, dataloader, device, dataset_size, numClasses):\n","    \n","    \"\"\"Evaluates the performance of the given model\"\"\"\n","    model.eval()\n","     \n","     #Dataset size\n","    numSamples = dataset_size\n","    \n","    # Create variables to store outputs and labels\n","    outputs_m=np.zeros((numSamples,numClasses),dtype=np.float)\n","    preds_m=np.zeros((numSamples,),dtype=np.int)\n","    #true_labels=np.zeros((numSamples,),dtype=np.int)\n","    contSamples=0\n","    #Batch Size\n","#    batchSize = labels.shape[0]\n","    with torch.no_grad():\n","         for samples in dataloader:\n","            inputs = samples['image'].to(device).float()\n","            #labels = samples['label'].to(device)\n","            \n","    \n","            outputs = model(inputs)\n","            outputs=F.softmax(outputs.data,dim=1)\n","            _, preds = torch.max(outputs, 1)\n","#            print('%d out of %d at batch %d'%(correct,total,i))\n","            \n","            outputs_m [contSamples:contSamples+batchSize,...]=outputs.cpu().numpy()\n","            preds_m [contSamples:contSamples+batchSize]=preds.cpu().numpy()\n","            #true_labels [contSamples:contSamples+batchSize]=labels.cpu().numpy()\n","            contSamples+=batchSize\n","    \n","    return preds_m, outputs_m #, true_labels\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Pq6lWpBC5Azo"},"source":["### Fine-tuning of a pre-trained CNN\n","Once we have defined the training and evaluation functions, we will fine-tune the resnet-18 CNN using our database. In addition, we define the loss, the optimizer and the lr scheduler:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zOYqyNca5Azo","colab":{}},"source":["#RESNET 18\n","\n","#model_ft = models.resnet18(pretrained=True)\n","\n","#num_ftrs = model_ft.fc.in_features\n","\n","# We need to set-up the output layer (fully connected) to provide 3 scores (nevus, melanoma, y queratosis).\n","#model_ft.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n","\n","#COnvert netowrk to GPU if available\n","#model_ft = model_ft.to(device)\n","\n","#The loss is a cross-entropy loss\n","#criterion = nn.CrossEntropyLoss()\n","\n","# We will use SGD with momentum as optimizer\n","#optimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9)\n","\n","# Our scheduler starts with an lr=1e-3 and decreases by a factor of 0.1 every 7 epochs.\n","#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6aC3F2VdmGhr","colab":{}},"source":["#RESNET 152\n","\n","#model_ft = models.resnet152(pretrained=True)\n","\n","#num_ftrs = model_ft.fc.in_features\n","\n","# We need to set-up the output layer (fully connected) to provide 3 scores (nevus, melanoma, y queratosis).\n","#model_ft.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n","\n","#COnvert netowrk to GPU if available\n","#model_ft = model_ft.to(device)\n","\n","#The loss is a cross-entropy loss\n","#criterion = nn.CrossEntropyLoss()\n","\n","# We will use SGD with momentum as optimizer\n","#optimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9)\n","\n","# Our scheduler starts with an lr=1e-3 and decreases by a factor of 0.1 every 7 epochs.\n","#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oq7X0gSc5Azr"},"source":["### Set-up the data loaders\n","No we will assign the dataloaders over the training and validation data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1cTfIO2p5Azr","colab":{}},"source":["#image_datasets = {'train' : train_dataset, 'val': val_dataset}\n","\n","#dataloaders = {'train' : train_dataloader, 'val': val_dataloader}\n","          \n","#dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n","#class_names = image_datasets['train'].classes\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xP2Kr4un5Azt"},"source":["### Train our network"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1JsVyGA-5Azt","scrolled":true,"colab":{}},"source":["#model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, image_datasets,dataset_sizes, dataloaders, device, num_epochs=25)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fWw-Yv88m6I9","colab":{}},"source":["#torch.save(model_ft.state_dict(), 'resnet152_1.pth')\n","#files.download('resnet152_1.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JNHk-9Dcy-Gp","colab":{}},"source":["#pred_labels, scores, true_labels =  get_results(model_ft, test_batch, test_dataloader, \n","#                                               device, len(test_dataset), len(class_names))\n","\n","#aucs = computeAUCs(scores, true_labels)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TLcRpdUjLgEu","colab":{}},"source":["#print('AUC %s: %.4f'%(class_names[1],aucs[1]))\n","#print('AUC %s: %.4f'%(class_names[2],aucs[2]))\n","\n","#print('AUC average', np.mean(aucs[1:]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VPWb-JcQYlS","colab_type":"code","colab":{}},"source":["tstval_batch = 256\n","\n","tr_batch_sizes = [8]\n","poss_lr = [1e-3]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WVKGpZF5h0v7","colab":{}},"source":["#tr_dataset, tst_dataset = getDermoData(tr_path, tst_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1589201894713,"user_tz":-120,"elapsed":7767622,"user":{"displayName":"Ignacio Serrano LLabrés","photoUrl":"","userId":"08869020757884954320"}},"id":"ImpcfuDuXGWU","outputId":"84b2f747-4b26-42d7-c16b-aa2271a091a9","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["da26c8c554b246508a59af527f1c22b5","4ab58fa0566643e586a6153c1f608b81","c3a8c24d9a8c488b914f0b1cbe3d4c12","495d8472cbcd49edbc71abbc507c900a","4d2286f59d66406d937dabdfb7491252","790bff0575e34d7f9452622d4f4bb848","4f400d98ccef4532b7077ace8f51b324","8a8824cf75e345398b648cf3f50b2431"]}},"source":["for i, tr_batch in enumerate(tr_batch_sizes):\n","\n","  print('Trainning batch size: ', tr_batch)\n","\n","  db = pd.read_csv('data/dermoscopyDBtrain.csv',header=0,dtype={'id': str, 'label': int})\n","  target = db.label.to_numpy()\n","  cls_weights = torch.from_numpy(compute_class_weight('balanced', np.unique(target), target))\n","  weights = cls_weights[target]\n","  sampler = WeightedRandomSampler(weights, len(target), replacement=True)\n","  \n","  ##Specify training dataset, with a batch size of 8, shuffle the samples, and parallelize with 4 workers\n","  train_dataloader = DataLoader(train_dataset, batch_size=tr_batch, sampler=sampler, drop_last=True)\n","\n","  #Validation dataset => No shuffle\n","  val_dataloader = DataLoader(val_dataset, batch_size=tstval_batch, shuffle=False)\n","\n","  #Test Dataset => => No shuffle\n","  test_dataloader = DataLoader(test_dataset, batch_size=tstval_batch, shuffle=False)\n","\n","  #tr_dataloader, val_dataloader, tst_dataloader = loadData(tr_path, tr_dataset, val_dataset, tst_dataset, tr_batch, tstval_batch)\n","\n","  image_datasets = {'train' : train_dataset, 'val': val_dataset}\n","  dataloaders = {'train' : train_dataloader, 'val': val_dataloader}            \n","  dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n","\n","  #model_ft = models.resnet152(pretrained=True)\n","  model_ft = models.googlenet(pretrained=True)\n","  model_ft.fc = nn.Linear(model_ft.fc.in_features, len(train_dataset.classes))\n","  model_ft = model_ft.to(device)\n","  criterion = nn.CrossEntropyLoss()\n","\n","  for j, learning_rate in enumerate(poss_lr):\n","\n","    print('Learning rate: ', learning_rate)\n","    \n","    optimizer_ft = optim.SGD(model_ft.parameters(), lr=learning_rate, momentum=0.9)\n","    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n","\n","    class_names = image_datasets['train'].classes\n","\n","    model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, image_datasets,dataset_sizes, dataloaders, device, num_epochs=50)\n","\n","    _, scores =  get_results(model_ft, tstval_batch, test_dataloader, device, len(test_dataset), len(class_names))\n","\n","    #aucs = computeAUCs(scores, true_labels)\n","\n","    #print('AUC %s: %.4f'%(class_names[1],aucs[1]))\n","    #print('AUC %s: %.4f'%(class_names[2],aucs[2]))\n","    #print('AUC average', np.mean(aucs[1:]))\n","\n","    #print('------------------------------------------------------------------------')\n","\n","    model_name = 'googleNet_batch_'+str(tr_batch)+ '_lr_' +str(learning_rate)+'.pth'\n","    csv_name = 'googleNet_batch_'+str(tr_batch)+ '_lr_' +str(learning_rate)+'.csv'\n","    np.savetxt(csv_name, scores, delimiter=\",\")\n","    torch.save(model_ft.state_dict(), model_name)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Trainning batch size:  8\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/checkpoints/googlenet-1378be20.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da26c8c554b246508a59af527f1c22b5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=52147035.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Learning rate:  0.001\n","Epoch 0/49\n","----------\n","train Loss: 0.8757 AUC mel: 0.7432 sk: 0.8379 avg: 0.7906\n","val Loss: 0.6754 AUC mel: 0.7767 sk: 0.9191 avg: 0.8479\n","\n","Epoch 1/49\n","----------\n","train Loss: 0.6626 AUC mel: 0.8583 sk: 0.9358 avg: 0.8971\n","val Loss: 0.6684 AUC mel: 0.7953 sk: 0.9312 avg: 0.8632\n","\n","Epoch 2/49\n","----------\n","train Loss: 0.5882 AUC mel: 0.8920 sk: 0.9490 avg: 0.9205\n","val Loss: 0.6028 AUC mel: 0.8147 sk: 0.9495 avg: 0.8821\n","\n","Epoch 3/49\n","----------\n","train Loss: 0.4852 AUC mel: 0.9219 sk: 0.9757 avg: 0.9488\n","val Loss: 0.6526 AUC mel: 0.8522 sk: 0.9162 avg: 0.8842\n","\n","Epoch 4/49\n","----------\n","train Loss: 0.4815 AUC mel: 0.9207 sk: 0.9743 avg: 0.9475\n","val Loss: 0.7284 AUC mel: 0.7856 sk: 0.9458 avg: 0.8657\n","\n","Epoch 5/49\n","----------\n","train Loss: 0.3827 AUC mel: 0.9492 sk: 0.9875 avg: 0.9683\n","val Loss: 0.7621 AUC mel: 0.7589 sk: 0.9129 avg: 0.8359\n","\n","Epoch 6/49\n","----------\n","train Loss: 0.3853 AUC mel: 0.9554 sk: 0.9812 avg: 0.9683\n","val Loss: 0.6713 AUC mel: 0.8075 sk: 0.9597 avg: 0.8836\n","\n","Epoch 7/49\n","----------\n","train Loss: 0.2759 AUC mel: 0.9743 sk: 0.9941 avg: 0.9842\n","val Loss: 0.6748 AUC mel: 0.8117 sk: 0.9546 avg: 0.8831\n","\n","Epoch 8/49\n","----------\n","train Loss: 0.2388 AUC mel: 0.9822 sk: 0.9949 avg: 0.9886\n","val Loss: 0.6386 AUC mel: 0.8064 sk: 0.9610 avg: 0.8837\n","\n","Epoch 9/49\n","----------\n","train Loss: 0.2346 AUC mel: 0.9806 sk: 0.9963 avg: 0.9884\n","val Loss: 0.6690 AUC mel: 0.7978 sk: 0.9568 avg: 0.8773\n","\n","Epoch 10/49\n","----------\n","train Loss: 0.2156 AUC mel: 0.9855 sk: 0.9968 avg: 0.9911\n","val Loss: 0.6307 AUC mel: 0.8122 sk: 0.9669 avg: 0.8896\n","\n","Epoch 11/49\n","----------\n","train Loss: 0.2227 AUC mel: 0.9865 sk: 0.9930 avg: 0.9898\n","val Loss: 0.7285 AUC mel: 0.8258 sk: 0.9696 avg: 0.8977\n","\n","Epoch 12/49\n","----------\n","train Loss: 0.1995 AUC mel: 0.9898 sk: 0.9954 avg: 0.9926\n","val Loss: 0.5970 AUC mel: 0.8286 sk: 0.9707 avg: 0.8996\n","\n","Epoch 13/49\n","----------\n","train Loss: 0.1823 AUC mel: 0.9912 sk: 0.9966 avg: 0.9939\n","val Loss: 0.6559 AUC mel: 0.7831 sk: 0.9579 avg: 0.8705\n","\n","Epoch 14/49\n","----------\n","train Loss: 0.1940 AUC mel: 0.9879 sk: 0.9962 avg: 0.9920\n","val Loss: 0.6816 AUC mel: 0.7828 sk: 0.9619 avg: 0.8723\n","\n","Epoch 15/49\n","----------\n","train Loss: 0.1946 AUC mel: 0.9875 sk: 0.9972 avg: 0.9924\n","val Loss: 0.6521 AUC mel: 0.8036 sk: 0.9689 avg: 0.8863\n","\n","Epoch 16/49\n","----------\n","train Loss: 0.1734 AUC mel: 0.9915 sk: 0.9976 avg: 0.9946\n","val Loss: 0.6847 AUC mel: 0.8025 sk: 0.9694 avg: 0.8859\n","\n","Epoch 17/49\n","----------\n","train Loss: 0.1778 AUC mel: 0.9911 sk: 0.9973 avg: 0.9942\n","val Loss: 0.6748 AUC mel: 0.8186 sk: 0.9691 avg: 0.8939\n","\n","Epoch 18/49\n","----------\n","train Loss: 0.1955 AUC mel: 0.9883 sk: 0.9958 avg: 0.9921\n","val Loss: 0.6830 AUC mel: 0.7878 sk: 0.9579 avg: 0.8728\n","\n","Epoch 19/49\n","----------\n","train Loss: 0.1899 AUC mel: 0.9885 sk: 0.9971 avg: 0.9928\n","val Loss: 0.6184 AUC mel: 0.8239 sk: 0.9716 avg: 0.8977\n","\n","Epoch 20/49\n","----------\n","train Loss: 0.1972 AUC mel: 0.9853 sk: 0.9986 avg: 0.9919\n","val Loss: 0.6018 AUC mel: 0.8144 sk: 0.9751 avg: 0.8948\n","\n","Epoch 21/49\n","----------\n","train Loss: 0.1777 AUC mel: 0.9899 sk: 0.9975 avg: 0.9937\n","val Loss: 0.6356 AUC mel: 0.8067 sk: 0.9669 avg: 0.8868\n","\n","Epoch 22/49\n","----------\n","train Loss: 0.1621 AUC mel: 0.9922 sk: 0.9984 avg: 0.9953\n","val Loss: 0.6022 AUC mel: 0.8083 sk: 0.9621 avg: 0.8852\n","\n","Epoch 23/49\n","----------\n","train Loss: 0.1825 AUC mel: 0.9892 sk: 0.9977 avg: 0.9935\n","val Loss: 0.6424 AUC mel: 0.8083 sk: 0.9700 avg: 0.8892\n","\n","Epoch 24/49\n","----------\n","train Loss: 0.1826 AUC mel: 0.9886 sk: 0.9977 avg: 0.9931\n","val Loss: 0.7557 AUC mel: 0.7811 sk: 0.9619 avg: 0.8715\n","\n","Epoch 25/49\n","----------\n","train Loss: 0.1901 AUC mel: 0.9876 sk: 0.9977 avg: 0.9927\n","val Loss: 0.6705 AUC mel: 0.8003 sk: 0.9705 avg: 0.8854\n","\n","Epoch 26/49\n","----------\n","train Loss: 0.1719 AUC mel: 0.9916 sk: 0.9976 avg: 0.9946\n","val Loss: 0.6712 AUC mel: 0.7931 sk: 0.9740 avg: 0.8835\n","\n","Epoch 27/49\n","----------\n","train Loss: 0.1644 AUC mel: 0.9918 sk: 0.9986 avg: 0.9952\n","val Loss: 0.7238 AUC mel: 0.7706 sk: 0.9632 avg: 0.8669\n","\n","Epoch 28/49\n","----------\n","train Loss: 0.1680 AUC mel: 0.9914 sk: 0.9982 avg: 0.9948\n","val Loss: 0.6649 AUC mel: 0.7969 sk: 0.9669 avg: 0.8819\n","\n","Epoch 29/49\n","----------\n","train Loss: 0.1683 AUC mel: 0.9912 sk: 0.9981 avg: 0.9947\n","val Loss: 0.6177 AUC mel: 0.8247 sk: 0.9733 avg: 0.8990\n","\n","Epoch 30/49\n","----------\n","train Loss: 0.1762 AUC mel: 0.9900 sk: 0.9976 avg: 0.9938\n","val Loss: 0.7025 AUC mel: 0.8117 sk: 0.9722 avg: 0.8919\n","\n","Epoch 31/49\n","----------\n","train Loss: 0.1766 AUC mel: 0.9903 sk: 0.9974 avg: 0.9939\n","val Loss: 0.6629 AUC mel: 0.8247 sk: 0.9672 avg: 0.8959\n","\n","Epoch 32/49\n","----------\n","train Loss: 0.1880 AUC mel: 0.9903 sk: 0.9975 avg: 0.9939\n","val Loss: 0.6959 AUC mel: 0.8006 sk: 0.9672 avg: 0.8839\n","\n","Epoch 33/49\n","----------\n","train Loss: 0.1704 AUC mel: 0.9918 sk: 0.9977 avg: 0.9948\n","val Loss: 0.6522 AUC mel: 0.8222 sk: 0.9806 avg: 0.9014\n","\n","Epoch 34/49\n","----------\n","train Loss: 0.1798 AUC mel: 0.9923 sk: 0.9960 avg: 0.9941\n","val Loss: 0.6436 AUC mel: 0.8117 sk: 0.9660 avg: 0.8889\n","\n","Epoch 35/49\n","----------\n","train Loss: 0.1687 AUC mel: 0.9908 sk: 0.9984 avg: 0.9946\n","val Loss: 0.6266 AUC mel: 0.8200 sk: 0.9705 avg: 0.8952\n","\n","Epoch 36/49\n","----------\n","train Loss: 0.1839 AUC mel: 0.9907 sk: 0.9967 avg: 0.9937\n","val Loss: 0.6615 AUC mel: 0.8058 sk: 0.9707 avg: 0.8883\n","\n","Epoch 37/49\n","----------\n","train Loss: 0.1774 AUC mel: 0.9902 sk: 0.9981 avg: 0.9942\n","val Loss: 0.6712 AUC mel: 0.8122 sk: 0.9696 avg: 0.8909\n","\n","Epoch 38/49\n","----------\n","train Loss: 0.1823 AUC mel: 0.9915 sk: 0.9959 avg: 0.9937\n","val Loss: 0.7033 AUC mel: 0.7947 sk: 0.9654 avg: 0.8801\n","\n","Epoch 39/49\n","----------\n","train Loss: 0.1644 AUC mel: 0.9913 sk: 0.9988 avg: 0.9950\n","val Loss: 0.7124 AUC mel: 0.7892 sk: 0.9724 avg: 0.8808\n","\n","Epoch 40/49\n","----------\n","train Loss: 0.1696 AUC mel: 0.9912 sk: 0.9979 avg: 0.9945\n","val Loss: 0.6698 AUC mel: 0.8069 sk: 0.9601 avg: 0.8835\n","\n","Epoch 41/49\n","----------\n","train Loss: 0.2033 AUC mel: 0.9871 sk: 0.9970 avg: 0.9921\n","val Loss: 0.7282 AUC mel: 0.7967 sk: 0.9738 avg: 0.8852\n","\n","Epoch 42/49\n","----------\n","train Loss: 0.1680 AUC mel: 0.9917 sk: 0.9977 avg: 0.9947\n","val Loss: 0.7333 AUC mel: 0.7933 sk: 0.9656 avg: 0.8795\n","\n","Epoch 43/49\n","----------\n","train Loss: 0.1910 AUC mel: 0.9894 sk: 0.9944 avg: 0.9919\n","val Loss: 0.6707 AUC mel: 0.7889 sk: 0.9665 avg: 0.8777\n","\n","Epoch 44/49\n","----------\n","train Loss: 0.1803 AUC mel: 0.9886 sk: 0.9978 avg: 0.9932\n","val Loss: 0.6296 AUC mel: 0.8122 sk: 0.9700 avg: 0.8911\n","\n","Epoch 45/49\n","----------\n","train Loss: 0.2009 AUC mel: 0.9879 sk: 0.9964 avg: 0.9921\n","val Loss: 0.7075 AUC mel: 0.7903 sk: 0.9581 avg: 0.8742\n","\n","Epoch 46/49\n","----------\n","train Loss: 0.1893 AUC mel: 0.9892 sk: 0.9972 avg: 0.9932\n","val Loss: 0.6589 AUC mel: 0.8039 sk: 0.9647 avg: 0.8843\n","\n","Epoch 47/49\n","----------\n","train Loss: 0.1639 AUC mel: 0.9913 sk: 0.9986 avg: 0.9949\n","val Loss: 0.6617 AUC mel: 0.8114 sk: 0.9658 avg: 0.8886\n","\n","Epoch 48/49\n","----------\n","train Loss: 0.1814 AUC mel: 0.9914 sk: 0.9979 avg: 0.9946\n","val Loss: 0.6995 AUC mel: 0.7900 sk: 0.9683 avg: 0.8791\n","\n","Epoch 49/49\n","----------\n","train Loss: 0.1639 AUC mel: 0.9927 sk: 0.9983 avg: 0.9955\n","val Loss: 0.6737 AUC mel: 0.7922 sk: 0.9656 avg: 0.8789\n","\n","Training complete in 127m 49s\n","Best val AUCs: mel 0.822222 sk 0.980600 avg 0.901411\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sbarVBWe5Azv"},"source":["## Part 3: Evaluation (Important)\n","The evaluation of this practice will be done through a challenge. For this, students are asked the following to provide an output matrix for both the validation and test database using this code. The matrix will have a size 600x3, 600 lesions and the 3 classes considered in the problem. The matrix must be provided in csv format (with 3 numbers per row separated by ',').\n","\n","In addition, students will submit a short report (1 side for the description, 1 side for references and figures if necessary) where they will describe the most important aspects of the proposed solution and include a table with the validation results achieved by their extensions/decisions. The objective of this report is for the teacher to assess the developments / extensions / decisions made by the students when optimizing their system. And the table is asked to demonstrate that, at least in validation, their decisions helped to improve the system performance.  You don't need to provide an absolute level of detail about the changes made, just list them, briefly discuss their purpose and show their impact in the table.\n","\n","The deadline for delivery of the results file and the report is Monday May 11 at 22:00.\n","\n"]}]}